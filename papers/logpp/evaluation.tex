% !TeX root = Logpp.tex
Given the implementation of \projn from \autoref{sec:implementation} this section
focuses on evaluating the resulting system and how it meets the design goals outlined in \autoref{sec:design}.\\

For the evaluation we use four microbenchmarks, listed below, which are each run for $10$k iterations. 
We also use a server based on the popular \emph{express}~\cite{express} framework 
which provides a \emph{REST} API for querying data on S\&P 500 companies.
All of the benchmarks are run on an Intel Xeon E5-1650 CPU with $6$ cores at $3.50$GHz, 
$32$GB of RAM, and SSD. The software stack is Windows 10 (17134) and Node v10.0.\\


\begin{lstlisting}[language=JavaScript,basicstyle=\scriptsize,numbers=none]
//Basic
log.info("hello world -- logger")

//String
log.info("hello %s", "world")

//Compound
log.info("hello %s %j %d", "world", { obj: true }, 4)

//Compute
log.info("hello at %j with %j %n -- %s", 
         new Date(), ["i", { f: i, g: "#" + i }], 
         i - 5, (i % 2 === 0 ? "ok" : "skip"))
\end{lstlisting}

\subsection{Microbenchmarks}
Our first evaluation is with current state of the art Node.js logging approaches. 
These include the builtin \texttt{console} methods, the \texttt{debug}~\cite{debuglogger} 
logger, the \texttt{bunyan}~\cite{bunyan} logger, and the \texttt{pino}~\cite{pino} logger.
Each benchmark was run 10 times discarding the highest and lowest times and reporting the 
average of the remaining runs.

\begin{table}[t]  
    \centering
    \caption{\small Timings for each logging framework on $10$k iterations with given format. 
    Speedup is the min-max speedup relative to the other logging frameworks.}
    {\small
    \begin{tabular}{l | r r r r }
    Program       & \bench{Basic}  & \bench{String}   & \bench{Compound}  & \bench{Compute} \\
    \hline
    Console       & $883$ ms & $852$ ms & $1064$ ms & $1239$ ms \\
    Debug         & $202$ ms & $200$ ms & $282$ ms  & $469$ ms \\
    Bunyan        & $477$ ms & $531$ ms & $603$ ms  & $920$ ms \\
    Pino             & $188$ ms & $190$ ms & $296$ ms  & $630$ ms \\
    Log++         & $89$ ms  & $93$ ms  & $155$ ms  & $304$ ms \\
    \hline
    Speedup & $2.1$-$9.9\times$ & $2.0$-$9.2\times$ & $1.8$-$6.9\times$ & $2.1$-$4.1\times$ \\
    \end{tabular}
    }
    \label{tab:microcompare}
\end{table}

The results in \autoref{tab:microcompare} show the wide performance variation across logging 
frameworks (spanning nearly a factor of $10\times$). Across all benchmarks \projn is consistently 
the fastest logger, by a factor of at least $1.8$-$2.1\times$, when compared to the best performing 
of the existing logging frameworks.

\subsection{Logging Optimization Impacts}
To understand how much each of our design choices and optimizations contributed to 
this performance we look at the performance impacts of specific features in \projn. 
\autoref{tab:featureeval} shows the \projn baseline, the logger when we disable the 
background formatting thread (\emph{Sync-Lazy}), the logger when we disable background 
formatting and disable batching of log messages in the in-memory buffer (\emph{Sync-Strict}). 
We also look at how discarding before formatting and disabling log statements via multi-level 
logging features impacts performance. The \emph{Levels (50\%)} row shows the performance 
when $50\%$ of the log statements are at the \texttt{INFO} level and $50\%$ are at the 
\emph{DETAIL} level (processed into the in-memory buffer but discarded before format). 
The \emph{Levels (33\%)} row shows the performance when $33\%$ of the log statements are 
at the \texttt{INFO} level, $33\%$ are at the \emph{DETAIL} level, and $33\%$ are at the \emph{DEBUG} 
level (entirely disabled).

\begin{table}[t]  
    \centering
    \caption{\small Baseline performance in \emph{Log++} row with disabled background 
    format in \emph{Sync-Lazy} and disabled background format \& disabled lazy 
    batch processing in \emph{Sync-Strict}. The \emph{Levels (50\%)} and 
    \emph{Levels (33\%)} rows show performance when $50\%$ of log messages are 
    in-memory level only and when $33\%$ are in-memory only and $33\%$ are entirely 
    disabled respectively.}
    {\small
    \begin{tabular}{l | r r r r }
    Program       & \bench{Basic}  & \bench{String}   & \bench{Compound}  & \bench{Compute} \\
    \hline
    Log++         & $89$ ms  & $93$ ms  & $155$ ms  & $304$ ms  \\
    Sync-Lazy     & $220$ ms & $216$ ms & $389$ ms  & $636$ ms  \\
    Sync-Strict   & $659$ ms & $788$ ms & $1035$ ms & $1323$ ms \\
    Levels (50\%) & $67$ ms  & $72$ ms  & $137$ ms  & $223$ ms  \\
    Levels (33\%) & $61$ ms  & $65$ ms  & $129$ ms  & $189$ ms  \\
    \end{tabular}
    }
    \label{tab:featureeval}
\end{table}

The results in the \emph{Sync-Lazy} and \emph{Sync-Strict} rows from 
\autoref{tab:featureeval} show the impacts of the background formatting and 
the in-memory batching. Disabling the background formatter thread shows that the 
speedup by offloading formatting to the background is substantial (around 
$2\times$ on the benchmarks). However, the impact of disabling the batching and 
lazy processing of the in-memory buffer is also significant. In our benchmarks we 
see as much as another $3.6\times$ slowdown. 

The use of \emph{expando} macros in the formats, \autoref{subsec:jsimpl}, can also play a 
large role in improving the performance of logging. \autoref{tab:expando} examines the 
performance difference when logging values using these macros vs. the cost of manually 
computing, formatting, and logging them.

\begin{table}[t]  
    \centering
    \caption{\small Comparison of log messages using \emph{expando} formats vs. manual computation 
    and formatting of values for \emph{hostname}, \emph{application name}, \emph{wall time}, and 
    a monotonic \emph{timestamp}}.
    {\small
    \begin{tabular}{l | r r r r }
    Program       & \bench{Host}  & \bench{App}   & \bench{Wallclock}  & \bench{Timestamp} \\
    \hline
    Explicit      & $8533$ ms & $65$ ms & $192$ ms & $41$ ms \\
    Expando       & $33$ ms   & $32$ ms & $41$ ms  & $36$ ms \\
    \end{tabular}
    }
    \label{tab:expando}
\end{table}

As seen in \autoref{tab:expando}, in cases such as adding the \emph{hostname} of the current machine 
or the very common desire of including the current date/time (\emph{wallclock}), there can be huge 
performance gains, $258\times$ and $4.7\times$ respectively, when using the expando macros. In other cases, 
such as the name of the current \emph{app} or a monotonic \emph{timestamp} value, the performance 
gains are a smaller but non-trivial $51\%$ and $12\%$. Instead the benefit is primarily in simpler/cleaner
logging code.

\subsection{Logging Performance}
The previous sections evaluated the performance of \projn with respect to other 
loggers on core logging tasks and explored the impacts of various design 
choices using microbenchmarks. This section evaluates the impact of logging on 
a lightweight \emph{REST} API service that supports querying data on S\&P 500 
companies. 
We use \emph{autocannon}~\cite{autocannon} in the default load generation 
setting to create a consistent load for a 10 second run on the service.
For comparison we include the builtin \texttt{console} methods 
and the \texttt{pino}~\cite{pino} logger in addition to \projn in the default 
setting.

This application highlights the tension between using logging as a telemetry source 
vs. a diagnostic tool. We updated it to use two logging levels, \texttt{DETAIL} 
and \texttt{INFO}. In the default runs we log at both levels and include a case, 
\emph{levels}, where \projn logs in-memory for the higher detail level but only 
emits at the lower level.

\begin{table}[t]  
    \centering
    \caption{\small Logging performance on the \emph{REST} service server for \texttt{console.log}, 
    \texttt{pino} and \texttt{Log++}. Also modified to take advantage of the \texttt{Log++} 
    multi-level logging functionality in the \emph{Log++ (levels)} row. Average and 
    standard deviation values for response latencies are shown along with the average 
    number of requests served per second.}
    {\small
    \begin{tabular}{l | r r r }
    Logger       & Latency (avg) & Latency (stdev) & Req./s (avg) \\
    \hline
%    No Logging     & $0.14$ms & $0.55$ms & $12,011$ \\
    Console        & $1.18$ ms & $0.83$ ms & $6668$   \\
    Pino           & $0.89$ ms & $0.70$ ms & $8133$   \\
    Log++          & $0.67$ ms & $0.80$ ms & $8645$   \\
    Log++ (levels) & $0.58$ ms & $0.77$ ms & $8958$   \\
    \end{tabular}
    }
    \label{tab:server}
\end{table}

The results in \autoref{tab:server} show that using a logging framework designed 
for modern development needs and built with performance in mind can have a 
substantial impact on an application. In terms of responses processed per second 
\projn increases the server throughput by $30\%$ from $6668$ to $8645$ requests 
per second. Further, \projn decreases the response time by $43\%$ 
from $1.18$ms to $0.67$ms. Despite using buffers and batched processing, 
which could in theory increase variability of the response latency, the standard deviation of 
the responses actually decreases slightly as well.

The results in \autoref{tab:server} also show that, in addition to the improvements 
seen by using \projn as a drop in replacement, it is possible to further improve the 
logger behavior by refactoring the logging statements to take advantage of the 
multi-level logging capabilities. For the \emph{Log++ (levels)} row the application is 
changed to write log statements that are relevant for debugging, but not for general 
telemetry, at the \texttt{DETAIL} level. This results in their being stored in 
the in-memory buffer, if needed for diagnostics, but not formatted and emitted. As 
a result the throughput increases a further $4\%$ to $8958$ and the latency goes 
down an additional $13\%$ to $0.58$ms on average.

\subsection{Logging Data Size}
The final metric we evaluate is how \projn can be used to reduce the amount of 
storage and network capacity consumed by logging data. \autoref{tab:compress} 
shows the log size generated per second when running the server benchmark with compression 
enabled (the \emph{Compressed} column) as well as the impact of multi-level 
logging not needed to format/emit all log data.

\begin{table}[t]  
    \centering
    \caption{\small Log data generated per second on the \emph{REST} service server for \texttt{Log++}
    and modified to take advantage of the \texttt{Log++} 
    multi-level logging functionality in the \emph{Log++ (levels)} row. Raw 
    logging output size (\emph{Raw} column) and log data size after deflate 
    compression (\emph{Compressed} column).}
    {\small
    \begin{tabular}{l | r r }
    Logger        & Raw & Compressed \\
    \hline
    Log++          & $2540$ kb/s & $137$ kb/s \\
    Log++ (levels) & $1176$ kb/s & $84$ kb/s   \\
    \end{tabular}
    }
    \label{tab:compress}
\end{table}

As shown in \autoref{tab:compress} both compression and the ability to 
discard detailed (and noisy) messages in the multi-level setup provide 
large reductions in the data sizes that need to be transmitted and stored. 
As expected compression is very effective on log data, reducing the size 
by $94.5\%$ from $2.54$~MB/s to $0.13$~MB/s. The ability to discard noisy messages, 
once it is determined they are not interesting for debugging, also has a 
large individual impact and reduces the data size by $53.7\%$. Combined these 
two optimizations result in a total data size reduction of a massive $96.7\%$ 
going from $2.54$~MB/s to just $0.08$~MB/s.
